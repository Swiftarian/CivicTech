import requests
import json
import re
import base64
import pandas as pd
from pathlib import Path

# Ollama API è¨­å®š
OLLAMA_CHAT_URL = "http://localhost:11434/api/chat"
OLLAMA_GENERATE_URL = "http://localhost:11434/api/generate"
DEFAULT_TEXT_MODEL = "llama3"
DEFAULT_VISION_MODEL = "llama3.2-vision"

def is_ollama_available():
    """æª¢æŸ¥ Ollama æœå‹™æ˜¯å¦é‹ä½œä¸­"""
    try:
        response = requests.get("http://localhost:11434/", timeout=2)
        return response.status_code == 200
    except:
        return False

def check_vision_model_available(model_name=DEFAULT_VISION_MODEL):
    """æª¢æŸ¥æŒ‡å®šçš„ Vision æ¨¡å‹æ˜¯å¦å¯ç”¨"""
    if not is_ollama_available():
        return False
    
    try:
        # å˜—è©¦åˆ—å‡ºå¯ç”¨æ¨¡å‹
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            return any(model_name in m['name'] for m in models)
        return False
    except:
        return False

def image_to_base64(image_path):
    """å°‡åœ–ç‰‡æª”æ¡ˆè½‰æ›ç‚º base64 ç·¨ç¢¼"""
    with open(image_path, 'rb') as img_file:
        return base64.b64encode(img_file.read()).decode('utf-8')

def classify_page_with_vision(image_path, model=DEFAULT_VISION_MODEL):
    """
    ä½¿ç”¨ Vision AI è¾¨è­˜é é¢é¡å‹
    
    Args:
        image_path (str): åœ–ç‰‡æª”æ¡ˆè·¯å¾‘
        model (str): Vision æ¨¡å‹åç¨±
        
    Returns:
        str: æ–‡ä»¶é¡å‹ (ä¾‹å¦‚: "æª¢ä¿®ç”³å ±æ›¸", "æª¢ä¿®ç›®éŒ„", "æœªçŸ¥é é¢")
    """
    try:
        # å°‡åœ–ç‰‡è½‰ç‚º base64
        img_base64 = image_to_base64(image_path)
        
        # æ§‹å»º prompt
        prompt = """é€™æ˜¯ä¸€ä»½æ¶ˆé˜²ç”³å ±æ–‡ä»¶çš„æƒæåœ–ã€‚è«‹è¾¨è­˜é€™é æœ€ä¸Šæ–¹çš„æ¨™é¡Œï¼ˆé€šå¸¸åœ¨å‰ 30% å€åŸŸï¼‰ï¼Œåˆ¤æ–·é€™æ˜¯ä»€éº¼æ–‡ä»¶ï¼Ÿ

è«‹åªå›å‚³æ–‡ä»¶åç¨±ï¼ˆä¾‹å¦‚ï¼š'æª¢ä¿®ç”³å ±æ›¸'ã€'æª¢ä¿®ç›®éŒ„'ã€'å¹³é¢åœ–'ã€'æ»…ç«å™¨æª¢æŸ¥è¡¨'ã€'æ¶ˆé˜²æ “æª¢æŸ¥è¡¨'ï¼‰ï¼Œä¸è¦å›å‚³å…¶ä»–èªªæ˜æ–‡å­—ã€‚

å¦‚æœç„¡æ³•ç¢ºå®šï¼Œè«‹å›å‚³ 'æœªçŸ¥é é¢'ã€‚"""
        
        # å‘¼å« Ollama Chat API (æ”¯æ´ vision)
        payload = {
            "model": model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt,
                    "images": [img_base64]
                }
            ],
            "stream": False
        }
        
        response = requests.post(OLLAMA_CHAT_URL, json=payload, timeout=60)
        
        if response.status_code == 200:
            result = response.json()
            doc_type = result['message']['content'].strip()
            # ç§»é™¤å¯èƒ½çš„å¼•è™Ÿ
            doc_type = doc_type.strip('"\'')
            return doc_type
        else:
            return "æœªçŸ¥é é¢"
            
    except Exception as e:
        print(f"Vision AI è¾¨è­˜å¤±æ•—: {e}")
        return "æœªçŸ¥é é¢"

def extract_checked_items_with_vision(image_path, model=DEFAULT_VISION_MODEL):
    """
    ä½¿ç”¨ Vision AI åµæ¸¬ç›®éŒ„é çš„å‹¾é¸é …ç›®
    
    Args:
        image_path (str): ç›®éŒ„é åœ–ç‰‡è·¯å¾‘
        model (str): Vision æ¨¡å‹åç¨±
        
    Returns:
        list: å·²å‹¾é¸çš„é …ç›®åˆ—è¡¨
    """
    try:
        img_base64 = image_to_base64(image_path)
        
        # å¼·åˆ¶çµæ§‹åŒ–è¼¸å‡ºçš„ prompt
        # å¼·åˆ¶çµæ§‹åŒ–è¼¸å‡ºçš„ prompt
        prompt = """é€™æ˜¯ä¸€å¼µæª¢ä¿®é …ç›®æ¸…å–®ã€‚è«‹ä»”ç´°è§€å¯Ÿæ¯ä¸€é …å‰é¢çš„æ–¹æ¡† (â–¡)ã€‚

è«‹åˆ—å‡ºæ‰€æœ‰ã€æ–¹æ¡†å…§æœ‰æ‰“å‹¾ (âœ“, v)ã€‘ã€ã€è¢«å¡—é»‘ã€‘æˆ–ã€æœ‰ä»»ä½•æ‰‹å¯«æ¨™è¨˜ã€‘çš„é …ç›®åç¨±ã€‚

è¦å‰‡ï¼š
1. åªè¦æ–¹æ¡†å…§ä¸æ˜¯ç©ºç™½çš„ï¼Œå°±è¦–ç‚ºå·²å‹¾é¸ã€‚
2. å¦‚æœæ–¹æ¡†è¢«å¡—æ»¿é»‘è‰²ï¼Œè¦–ç‚ºå·²å‹¾é¸ã€‚
3. å¦‚æœæ–¹æ¡†å…§æœ‰æ‰“å‹¾æˆ–æ‰“å‰ï¼Œè¦–ç‚ºå·²å‹¾é¸ã€‚
4. å¿½ç•¥å®Œå…¨ç©ºç™½çš„æ–¹æ¡†ã€‚

IMPORTANT: Do NOT output any markdown, explanations, or code blocks. Only output a valid JSON array of strings.

Example: ["æ»…ç«å™¨", "é¿é›£å™¨å…·", "ç«è­¦è‡ªå‹•è­¦å ±è¨­å‚™"]

å¦‚æœæ²’æœ‰ä»»ä½•é …ç›®è¢«å‹¾é¸ï¼Œè«‹å›å‚³ç©ºé™£åˆ—: []"""
        
        payload = {
            "model": model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt,
                    "images": [img_base64]
                }
            ],
            "stream": False
        }
        
        response = requests.post(OLLAMA_CHAT_URL, json=payload, timeout=60)
        
        if response.status_code == 200:
            result = response.json()
            content = result['message']['content'].strip()
            
            # ä½¿ç”¨ Regex æå– JSON é™£åˆ—ï¼ˆé˜²æ­¢æ¨¡å‹è¼¸å‡ºå¤šé¤˜æ–‡å­—ï¼‰
            json_match = re.search(r'\[.*\]', content, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                checked_items = json.loads(json_str)
                return checked_items
            else:
                # å¦‚æœæ‰¾ä¸åˆ° JSONï¼Œå›å‚³ç©ºåˆ—è¡¨
                print(f"ç„¡æ³•å¾å›æ‡‰ä¸­æå– JSON: {content}")
                return []
        else:
            return []
            
    except Exception as e:
        print(f"å‹¾é¸é …ç›®æå–å¤±æ•—: {e}")
        return []

def analyze_document_structure(pdf_images_or_path, model=DEFAULT_VISION_MODEL):
    """
    å®Œæ•´çš„æ–‡ä»¶çµæ§‹åˆ†æ (4-step æµç¨‹)
    
    Args:
        pdf_images_or_path: å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€:
            - list of PIL Images
            - list of image file paths
            - PDF file path (æœƒè‡ªå‹•è½‰æ›ç‚ºåœ–ç‰‡)
        model (str): Vision æ¨¡å‹åç¨±
        
    Returns:
        dict: {
            'page_map': {é ç¢¼: æ–‡ä»¶é¡å‹},
            'toc_page': ç›®éŒ„é ç¢¼ (or None),
            'required_items': [å·²å‹¾é¸é …ç›®åˆ—è¡¨],
            'validation_report': pd.DataFrame,
            'error': error message (if any)
        }
    """
    # ç’°å¢ƒæª¢æŸ¥
    if not is_ollama_available():
        raise RuntimeError("Ollama æœå‹™æœªå•Ÿå‹•ï¼Œè«‹å…ˆåŸ·è¡Œ 'ollama serve' æˆ–å•Ÿå‹• Ollama Desktop")
    
    if not check_vision_model_available(model):
        raise RuntimeError(f"Vision æ¨¡å‹ '{model}' æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: ollama pull {model}")
    
    # è™•ç†è¼¸å…¥ (å¦‚æœæ˜¯ PDF è·¯å¾‘ï¼Œéœ€è¦è½‰æ›ç‚ºåœ–ç‰‡)
    # é€™è£¡å‡è¨­å·²ç¶“ç”±èª¿ç”¨æ–¹è½‰æ›å¥½ï¼ˆå› ç‚ºä¸»ç¨‹å¼å·²æœ‰è½‰æ›é‚è¼¯ï¼‰
    if isinstance(pdf_images_or_path, (str, Path)):
        # å¦‚æœå‚³å…¥çš„æ˜¯ PDF pathï¼Œé€™è£¡å¯ä»¥æ·»åŠ  pdf2image è½‰æ›é‚è¼¯
        # ä½†ç‚ºäº†ç°¡åŒ–ï¼Œæˆ‘å€‘å‡è¨­ä¸»ç¨‹å¼å·²è™•ç†å¥½åœ–ç‰‡åˆ—è¡¨
        raise NotImplementedError("è«‹å…ˆå°‡ PDF è½‰ç‚ºåœ–ç‰‡åˆ—è¡¨å†å‚³å…¥")
    
    images = pdf_images_or_path
    
    # çµæœå®¹å™¨
    result = {
        'page_map': {},
        'toc_page': None,
        'required_items': [],
        'validation_report': None,
        'error': None
    }
    
    try:
        # Step 1: é é¢è­˜åˆ¥ (Page Classification)
        print("ğŸ” Step 1: æ­£åœ¨é€²è¡Œé é¢è­˜åˆ¥...")
        for i, img in enumerate(images):
            page_num = i + 1
            
            # å¦‚æœæ˜¯ PIL Imageï¼Œéœ€è¦å…ˆå„²å­˜ç‚ºè‡¨æ™‚æª”æ¡ˆ
            if hasattr(img, 'save'):
                import os
                import tempfile
                temp_dir = tempfile.gettempdir()
                temp_path = os.path.join(temp_dir, f"temp_page_{page_num}.png")
                img.save(temp_path)
                doc_type = classify_page_with_vision(temp_path, model)
                try:
                    os.remove(temp_path)
                except:
                    pass  # å¿½ç•¥åˆªé™¤éŒ¯èª¤
            else:
                # å‡è¨­æ˜¯æª”æ¡ˆè·¯å¾‘
                doc_type = classify_page_with_vision(img, model)
            
            result['page_map'][page_num] = doc_type
            print(f"  ç¬¬ {page_num} é : {doc_type}")
        
        # Step 2: åµæ¸¬ã€Œæª¢ä¿®ç›®éŒ„ã€èˆ‡å‹¾é¸é …ç›®
        print("\nğŸ“‹ Step 2: æ­£åœ¨å°‹æ‰¾ç›®éŒ„é ä¸¦æå–å‹¾é¸é …ç›®...")
        
        # å°‹æ‰¾ç›®éŒ„é 
        toc_keywords = ['ç›®éŒ„', 'æª¢ä¿®é …ç›®', 'ç”³å ±é …ç›®', 'æ¸…å–®']
        for page_num, doc_type in result['page_map'].items():
            if any(keyword in doc_type for keyword in toc_keywords):
                result['toc_page'] = page_num
                print(f"  âœ… æ‰¾åˆ°ç›®éŒ„é : ç¬¬ {page_num} é ")
                
                # æå–å‹¾é¸é …ç›®
                img = images[page_num - 1]
                if hasattr(img, 'save'):
                    import os
                    import tempfile
                    temp_dir = tempfile.gettempdir()
                    temp_path = os.path.join(temp_dir, "temp_toc.png")
                    img.save(temp_path)
                    required_items = extract_checked_items_with_vision(temp_path, model)
                    try:
                        os.remove(temp_path)
                    except:
                        pass
                else:
                    required_items = extract_checked_items_with_vision(img, model)
                
                result['required_items'] = required_items
                print(f"  æ‰¾åˆ° {len(required_items)} å€‹å‹¾é¸é …ç›®: {required_items}")
                break
        
        if not result['toc_page']:
            print("  âš ï¸ æœªæ‰¾åˆ°ç›®éŒ„é ")
        
        # Step 3 & 4: äº¤å‰æ¯”å° & ç”Ÿæˆå ±å‘Š
        print("\nâœ… Step 3 & 4: æ­£åœ¨é€²è¡Œäº¤å‰æ¯”å°ä¸¦ç”Ÿæˆå ±å‘Š...")
        
        report_data = []
        for item in result['required_items']:
            # åˆ¤å®šè¦å‰‡: åœ¨ page_map ä¸­å°‹æ‰¾åŒ…å«è©²é …ç›®åç¨±çš„é é¢
            found_pages = []
            for page_num, doc_type in result['page_map'].items():
                # æ¨¡ç³ŠåŒ¹é… (ä¾‹å¦‚ "æ»…ç«å™¨" æ‡‰è©²åŒ¹é… "æ»…ç«å™¨æª¢æŸ¥è¡¨")
                if item in doc_type or doc_type in item:
                    found_pages.append(page_num)
            
            status = "âœ… åˆè¦" if found_pages else "âŒ ç¼ºä»¶"
            page_list = ", ".join([f"ç¬¬{p}é " for p in found_pages]) if found_pages else "-"
            
            report_data.append({
                'æ‡‰æª¢é™„é …ç›®': item,
                'æ˜¯å¦å‹¾é¸': 'âœ“',
                'å¯¦éš›é æ•¸': page_list,
                'ç‹€æ…‹': status
            })
        
        result['validation_report'] = pd.DataFrame(report_data)
        
        print("  âœ… å ±å‘Šç”Ÿæˆå®Œæˆ")
        return result
        
    except Exception as e:
        result['error'] = str(e)
        print(f"âŒ åˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        return result

def analyze_page_with_ai(text_content, model=DEFAULT_TEXT_MODEL):
    """
    ä½¿ç”¨ AI åˆ†æå–®é å…§å®¹ (åŸºæ–¼æ–‡å­—çš„ OCR çµæœ)
    
    Args:
        text_content (str): OCR è¾¨è­˜å‡ºçš„æ–‡å­—
        model (str): ä½¿ç”¨çš„æ¨¡å‹åç¨±
        
    Returns:
        dict: AI åˆ†æçµæœ
    """
    if not text_content.strip():
        return {"error": "No text content"}

    prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ¶ˆé˜²å®‰å…¨æª¢æŸ¥å“¡ã€‚è«‹å¾ä»¥ä¸‹ OCR æ–‡å­—ä¸­æå–é—œéµè³‡è¨Šã€‚
    
    OCR æ–‡å­—:
    ----------------
    {text_content}
    ----------------
    
    ğŸ“Œ **æ ¸å¿ƒè¦å‰‡ï¼ˆæœ€é«˜å„ªå…ˆç´š - å¿…é ˆåš´æ ¼éµå®ˆï¼‰**ï¼š
    
    âš ï¸ **å‹¾é¸ç¬¦è™Ÿè­˜åˆ¥è¦å‰‡ï¼ˆé€™æ˜¯æœ€é‡è¦çš„è¦å‰‡ï¼ï¼‰**ï¼š
    1. åœ¨ç›®éŒ„é ï¼ˆã€Œæ¶ˆé˜²å®‰å…¨è¨­å‚™æª¢ä¿®ç”³å ±æ›¸ç›®éŒ„ã€ï¼‰ä¸­ï¼Œæ¯å€‹è¨­å‚™å‰é¢éƒ½æœ‰æ–¹æ¡†
    2. **ä¸»è¦åˆ¤æ–·ä¾æ“šï¼šæ–¹æ¡†å…§æœ‰æ‰“å‹¾ï¼ˆâœ“ã€â˜‘ã€âˆšã€âœ”ã€â– ã€â—ï¼‰çš„é …ç›®**
    3. **ğŸ’¡ ä¸Šä¸‹æ–‡æ¨æ–· (Context Inference) - å®¹éŒ¯æ©Ÿåˆ¶**ï¼š
       - å¦‚æœ OCR æ²’æœ‰è¾¨è­˜å‡ºæ–¹æ¡†æˆ–å‹¾é¸ç¬¦è™Ÿï¼Œä½†è©²é …ç›®å¾Œé¢**æœ‰å¡«å¯«æ•¸é‡ã€é ç¢¼æˆ–å‚™è¨»** (ä¾‹å¦‚ "æ»…ç«å™¨.....7" æˆ– "é¿é›£å™¨å…·...3å…·")ï¼Œ**è«‹è¦–ç‚ºå·²å‹¾é¸**ã€‚
       - é€™æ˜¯å› ç‚º OCR å¸¸æœƒæ¼æ‰æ–¹æ¡†ï¼Œä½†å¾Œé¢çš„æ•¸å­—é€šå¸¸ä»£è¡¨è©²è¨­å‚™æœ‰å¯¦éš›æª¢ä¿®ã€‚
    
    âœ… **æ­£ç¢ºç¯„ä¾‹**ï¼ˆæ‡‰è©²æå–ï¼‰ï¼š
    - "â˜‘ æ»…ç«å™¨" â†’ æå– "æ»…ç«å™¨" (æœ‰å‹¾é¸)
    - "âœ“ ç«è­¦è‡ªå‹•è­¦å ±è¨­å‚™" â†’ æå– "ç«è­¦è‡ªå‹•è­¦å ±è¨­å‚™" (æœ‰å‹¾é¸)
    - "æ»…ç«å™¨ .......... 3" â†’ æå– "æ»…ç«å™¨" (é›–ç„¡å‹¾é¸ï¼Œä½†æœ‰é ç¢¼/æ•¸é‡)
    - "é¿é›£å™¨å…· 5 å…·" â†’ æå– "é¿é›£å™¨å…·" (é›–ç„¡å‹¾é¸ï¼Œä½†æœ‰æ•¸é‡)
    
    âŒ **éŒ¯èª¤ç¯„ä¾‹**ï¼ˆçµ•å°ä¸è¦æå–ï¼‰ï¼š
    - "â˜ å®¤å¤–æ¶ˆé˜²æ “è¨­å‚™" â†’ **ä¸æå–**ï¼ˆæ˜ç¢ºçš„ç©ºç™½æ–¹æ¡†ï¼‰
    - "â–¡ æ’ç…™è¨­å‚™" â†’ **ä¸æå–**ï¼ˆæ˜ç¢ºçš„ç©ºç™½æ–¹æ¡†ï¼‰
    - "é€£çµé€æ°´ç®¡" â†’ **ä¸æå–**ï¼ˆç„¡å‹¾é¸ï¼Œä¸”å¾Œé¢ç„¡ä»»ä½•æ•¸å­—æˆ–å…§å®¹ï¼‰
    
    ğŸ’¡ **å¯¦éš›æ¡ˆä¾‹**ï¼š
    å¦‚æœ OCR æ–‡å­—é¡¯ç¤ºï¼š
    ```
    â˜‘ æ»…ç«å™¨ _____ 3
    â˜ å®¤å¤–æ¶ˆé˜²æ “è¨­å‚™ _____ 4
    ç«è­¦è‡ªå‹•è­¦å ±è¨­å‚™ ..... 7  <-- (OCR æ¼äº†å‹¾å‹¾ï¼Œä½†æœ‰é ç¢¼ 7)
    â–¡ æ’ç…™è¨­å‚™ _____ 17
    ```
    æ­£ç¢ºçš„ equipment_list æ‡‰è©²æ˜¯ï¼š["æ»…ç«å™¨", "ç«è­¦è‡ªå‹•è­¦å ±è¨­å‚™"]
    (å®¤å¤–æ¶ˆé˜²æ “å’Œæ’ç…™è¨­å‚™æ˜ç¢ºæœªå‹¾é¸ï¼Œæ‰€ä»¥ä¸æå–)
    
    ---
    
    è«‹æå–ä»¥ä¸‹æ¬„ä½ä¸¦ä»¥ JSON æ ¼å¼å›å‚³ã€‚
    
    âš ï¸ **å…¶ä»–é‡è¦è¦å‰‡**ï¼š
    0. **å¼·åˆ¶ä½¿ç”¨ç¹é«”ä¸­æ–‡**ï¼šæ‰€æœ‰è¼¸å‡ºå¿…é ˆä½¿ç”¨å°ç£ç¹é«”ä¸­æ–‡ï¼Œåš´ç¦ä½¿ç”¨ç°¡é«”å­—ï¼ˆä¾‹å¦‚ï¼šã€Œå°æ±ã€è€Œéã€Œå°ä¸œã€ã€ã€Œç¶±ã€è€Œéã€Œçº²ã€ï¼‰ã€‚
    1. **å»é™¤æ‰€æœ‰ç©ºæ ¼**ï¼šæ‰€æœ‰è¼¸å‡ºçš„å€¼éƒ½å¿…é ˆå»é™¤æ‰€æœ‰ç©ºæ ¼ (ä¾‹å¦‚ "é³³ ä»™" -> "é³³ä»™")ã€‚
    2. **å–®ä¸€å­—ä¸²**ï¼šåœ°å€å’Œç®¡ç†æ¬Šäººå¿…é ˆæ˜¯å–®ä¸€å­—ä¸²ï¼Œåš´ç¦ä½¿ç”¨å·¢ç‹€ JSON (ä¾‹å¦‚ä¸è¦å›å‚³ {{'city': ...}})ã€‚
    3. **OCR å®¹éŒ¯**ï¼šOCR å¯èƒ½æœ‰éŒ¯å­—ã€ç¼ºå­—ã€å¤šå­—æˆ–ç©ºæ ¼å•é¡Œï¼Œè«‹ä½¿ç”¨æ¨¡ç³Šæ¯”å°ï¼Œç›¸ä¼¼åº¦ 80% ä»¥ä¸Šå³å¯æ¥å—ã€‚
    
    æ¬„ä½èªªæ˜ï¼š
    1. document_type: æ–‡ä»¶é¡å‹
    2. place_name: å ´æ‰€åç¨± (å»é™¤ç©ºæ ¼)
    3. address: åœ°å€ (å®Œæ•´åœ°å€å­—ä¸²ï¼Œå»é™¤ç©ºæ ¼)
    4. management_person: ç®¡ç†æ¬Šäºº (å§“åå­—ä¸²ï¼Œå»é™¤ç©ºæ ¼)
    5. equipment_list: æ¶ˆé˜²è¨­å‚™åˆ—è¡¨ (Arrayï¼Œæ¯å€‹é …ç›®ä¹Ÿè¦å»é™¤ç©ºæ ¼)

    ğŸ“‹ **æ¨™æº–è¨­å‚™æ¸…å–®** (è«‹å„ªå…ˆå¾ä»¥ä¸‹æ¸…å–®ä¸­æ¯”å°ï¼Œä½¿ç”¨æ¨¡ç³Šæ¯”å°):
    - æ»…ç«å™¨
    - å®¤å…§æ¶ˆé˜²æ “è¨­å‚™
    - å®¤å¤–æ¶ˆé˜²æ “è¨­å‚™
    - è‡ªå‹•æ’’æ°´è¨­å‚™
    - æ°´éœ§æ»…ç«è¨­å‚™
    - æ³¡æ²«æ»…ç«è¨­å‚™
    - äºŒæ°§åŒ–ç¢³æ»…ç«è¨­å‚™
    - ä¹¾ç²‰æ»…ç«è¨­å‚™
    - æµ·é¾æ»…ç«è¨­å‚™(å«æµ·é¾æ›¿ä»£å“)
    - ç«è­¦è‡ªå‹•è­¦å ±è¨­å‚™
    - ç“¦æ–¯æ¼æ°£ç«è­¦è‡ªå‹•è­¦å ±è¨­å‚™
    - ç·Šæ€¥å»£æ’­è¨­å‚™
    - æ¨™ç¤ºè¨­å‚™
    - é¿é›£å™¨å…·
    - ç·Šæ€¥ç…§æ˜è¨­å‚™
    - é€£çµé€æ°´ç®¡
    - æ¶ˆé˜²å°ˆç”¨è“„æ°´æ± 
    - æ’ç…™è¨­å‚™
    - ç„¡ç·šé›»é€šä¿¡è¼”åŠ©è¨­å‚™
    
    ğŸ” **æ¨¡ç³Šæ¯”å°è¦å‰‡**ï¼š
    - OCR å¯èƒ½å°‡ã€Œå…§ã€è­˜åˆ¥ç‚ºã€Œå†…ã€ã€ã€Œæ “ã€è­˜åˆ¥ç‚ºã€Œæ‹´ã€
    - å¯èƒ½æœ‰å¤šé¤˜ç©ºæ ¼ï¼šã€Œå®¤ å…§ æ¶ˆ é˜² æ “ã€-> ã€Œå®¤å…§æ¶ˆé˜²æ “è¨­å‚™ã€
    - å¯èƒ½ç¼ºå°‘ã€Œè¨­å‚™ã€äºŒå­—ï¼šã€Œå®¤å…§æ¶ˆé˜²æ “ã€-> ã€Œå®¤å…§æ¶ˆé˜²æ “è¨­å‚™ã€
    - ç°¡é«”è½‰ç¹é«”ï¼šã€Œç­ç«å™¨ã€-> ã€Œæ»…ç«å™¨ã€
    - å…¨å½¢è½‰åŠå½¢ï¼šã€Œ(å«æµ·é¾æ›¿ä»£å“)ã€-> ã€Œ(å«æµ·é¾æ›¿ä»£å“)ã€
    
    OCR è¼¸å…¥: "å®¤ å†… æ¶ˆ é˜² æ‹´"
    æ­£ç¢ºè¼¸å‡º: "å®¤å…§æ¶ˆé˜²æ “è¨­å‚™"
    
    OCR è¼¸å…¥: "ç«è­¦è‡ªå‹•è­¦å ±"
    æ­£ç¢ºè¼¸å‡º: "ç«è­¦è‡ªå‹•è­¦å ±è¨­å‚™"

    å¦‚æœæ‰¾ä¸åˆ°æ¬„ä½ï¼Œè«‹å¡« nullã€‚åªå›å‚³ JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
    """
    
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    
    try:
        response = requests.post(OLLAMA_GENERATE_URL, json=payload, timeout=30)
        if response.status_code == 200:
            result = response.json()
            response_text = result['response']
            
            if not response_text or not response_text.strip():
                return {"error": "AI returned empty response"}

            # Debug: Print raw response
            print(f"ğŸ¤– AI Raw Response: {response_text}")

            # å˜—è©¦ä½¿ç”¨ Regex æå– JSON (è™•ç† Markdown æ¨™è¨˜)
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            
            try:
                if json_match:
                    return json.loads(json_match.group(0))
                else:
                    # å¦‚æœ Regex å¤±æ•—ï¼Œæª¢æŸ¥æ˜¯å¦åƒ JSON
                    clean_text = response_text.strip()
                    if not clean_text.startswith("{"):
                        return {"error": "No JSON object found in AI response", "raw_response": response_text}
                    return json.loads(clean_text)
            except json.JSONDecodeError as je:
                # JSON è§£æå¤±æ•—ï¼Œå›å‚³åŸå§‹æ–‡å­—ä¾›é™¤éŒ¯
                # å˜—è©¦ä¿®å¾©å¸¸è¦‹çš„ JSON éŒ¯èª¤ (ä¾‹å¦‚å–®å¼•è™Ÿ)
                try:
                    import ast
                    return ast.literal_eval(response_text)
                except:
                    pass
                return {"error": f"JSON Parse Error: {je}", "raw_response": response_text}
                
        else:
            return {"error": f"API Error: {response.status_code}"}
    except Exception as e:
        return {"error": str(e)}

def analyze_document(pages_text, model=DEFAULT_TEXT_MODEL):
    """
    åˆ†ææ•´ä»½æ–‡ä»¶ (å¤šé ) - åŸºæ–¼ OCR æ–‡å­—
    
    Args:
        pages_text (list): æ¯ä¸€é çš„ OCR æ–‡å­—åˆ—è¡¨
        
    Returns:
        dict: æ•´åˆå¾Œçš„åˆ†æçµæœ
    """
    if not is_ollama_available():
        return {"error": "Ollama service not available"}
        
    # é€™è£¡å¯ä»¥å¯¦ä½œæ›´è¤‡é›œçš„é‚è¼¯ï¼Œä¾‹å¦‚åªåˆ†æç¬¬ä¸€é ï¼Œæˆ–æ˜¯å½™æ•´æ‰€æœ‰é é¢
    # å„ªåŒ–ï¼šåŒæ™‚åˆ†æç¬¬ä¸€é (åŸºæœ¬è³‡æ–™)å’Œç›®éŒ„é (è¨­å‚™æ¸…å–®)
    if pages_text:
        combined_text = pages_text[0] # é è¨­åŒ…å«ç¬¬ä¸€é 
        
        # å°‹æ‰¾ç›®éŒ„é  (é—œéµå­—: ç›®éŒ„, é™„è¡¨, æª¢æŸ¥è¡¨)
        toc_keywords = ["ç›®éŒ„", "é™„è¡¨", "æª¢æŸ¥è¡¨"]
        toc_text = ""
        
        # å¾ç¬¬äºŒé é–‹å§‹æ‰¾ (index 1)
        if len(pages_text) > 1:
            for i in range(1, len(pages_text)):
                page_content = pages_text[i]
                # ç°¡å–®åˆ¤æ–·
                if any(kw in page_content for kw in toc_keywords):
                    toc_text = page_content
                    break
            
            # å¦‚æœæ²’æ‰¾åˆ°æ˜ç¢ºçš„ç›®éŒ„é ï¼Œä½†æœ‰ç¬¬äºŒé ï¼Œå°±é è¨­æŠ“ç¬¬äºŒé  (é€šå¸¸ç›®éŒ„åœ¨ç¬¬äºŒé )
            if not toc_text and len(pages_text) > 1:
                toc_text = pages_text[1]
        
        if toc_text:
            combined_text += "\n\n--- (ä»¥ä¸‹ç‚ºç›®éŒ„é å…§å®¹) ---\n\n" + toc_text
            
        return analyze_page_with_ai(combined_text, model)
    return {}
