import requests
import json
import re
import base64
import pandas as pd
from pathlib import Path

# Ollama API è¨­å®š
OLLAMA_CHAT_URL = "http://localhost:11434/api/chat"
OLLAMA_GENERATE_URL = "http://localhost:11434/api/generate"
DEFAULT_TEXT_MODEL = "llama3"
DEFAULT_VISION_MODEL = "llama3.2-vision"

def is_ollama_available():
    """æª¢æŸ¥ Ollama æœå‹™æ˜¯å¦é‹ä½œä¸­"""
    try:
        response = requests.get("http://localhost:11434/", timeout=2)
        return response.status_code == 200
    except:
        return False

def check_vision_model_available(model_name=DEFAULT_VISION_MODEL):
    """æª¢æŸ¥æŒ‡å®šçš„ Vision æ¨¡å‹æ˜¯å¦å¯ç”¨"""
    if not is_ollama_available():
        return False
    
    try:
        # å˜—è©¦åˆ—å‡ºå¯ç”¨æ¨¡å‹
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            return any(model_name in m['name'] for m in models)
        return False
    except:
        return False

def image_to_base64(image_path):
    """å°‡åœ–ç‰‡æª”æ¡ˆè½‰æ›ç‚º base64 ç·¨ç¢¼"""
    with open(image_path, 'rb') as img_file:
        return base64.b64encode(img_file.read()).decode('utf-8')

def classify_page_with_vision(image_path, model=DEFAULT_VISION_MODEL):
    """
    ä½¿ç”¨ Vision AI è¾¨è­˜é é¢é¡å‹
    
    Args:
        image_path (str): åœ–ç‰‡æª”æ¡ˆè·¯å¾‘
        model (str): Vision æ¨¡å‹åç¨±
        
    Returns:
        str: æ–‡ä»¶é¡å‹ (ä¾‹å¦‚: "æª¢ä¿®ç”³å ±æ›¸", "æª¢ä¿®ç›®éŒ„", "æœªçŸ¥é é¢")
    """
    try:
        # å°‡åœ–ç‰‡è½‰ç‚º base64
        img_base64 = image_to_base64(image_path)
        
        # æ§‹å»º prompt
        prompt = """é€™æ˜¯ä¸€ä»½æ¶ˆé˜²ç”³å ±æ–‡ä»¶çš„æƒæåœ–ã€‚è«‹è¾¨è­˜é€™é æœ€ä¸Šæ–¹çš„æ¨™é¡Œï¼ˆé€šå¸¸åœ¨å‰ 30% å€åŸŸï¼‰ï¼Œåˆ¤æ–·é€™æ˜¯ä»€éº¼æ–‡ä»¶ï¼Ÿ

è«‹åªå›å‚³æ–‡ä»¶åç¨±ï¼ˆä¾‹å¦‚ï¼š'æª¢ä¿®ç”³å ±æ›¸'ã€'æª¢ä¿®ç›®éŒ„'ã€'å¹³é¢åœ–'ã€'æ»…ç«å™¨æª¢æŸ¥è¡¨'ã€'æ¶ˆé˜²æ “æª¢æŸ¥è¡¨'ï¼‰ï¼Œä¸è¦å›å‚³å…¶ä»–èªªæ˜æ–‡å­—ã€‚

å¦‚æœç„¡æ³•ç¢ºå®šï¼Œè«‹å›å‚³ 'æœªçŸ¥é é¢'ã€‚"""
        
        # å‘¼å« Ollama Chat API (æ”¯æ´ vision)
        payload = {
            "model": model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt,
                    "images": [img_base64]
                }
            ],
            "stream": False
        }
        
        response = requests.post(OLLAMA_CHAT_URL, json=payload, timeout=60)
        
        if response.status_code == 200:
            result = response.json()
            doc_type = result['message']['content'].strip()
            # ç§»é™¤å¯èƒ½çš„å¼•è™Ÿ
            doc_type = doc_type.strip('"\'')
            return doc_type
        else:
            return "æœªçŸ¥é é¢"
            
    except Exception as e:
        print(f"Vision AI è¾¨è­˜å¤±æ•—: {e}")
        return "æœªçŸ¥é é¢"

def extract_checked_items_with_vision(image_path, model=DEFAULT_VISION_MODEL):
    """
    ä½¿ç”¨ Vision AI åµæ¸¬ç›®éŒ„é çš„å‹¾é¸é …ç›®
    
    Args:
        image_path (str): ç›®éŒ„é åœ–ç‰‡è·¯å¾‘
        model (str): Vision æ¨¡å‹åç¨±
        
    Returns:
        list: å·²å‹¾é¸çš„é …ç›®åˆ—è¡¨
    """
    try:
        img_base64 = image_to_base64(image_path)
        
        # å¼·åˆ¶çµæ§‹åŒ–è¼¸å‡ºçš„ prompt
        prompt = """é€™æ˜¯ä¸€å¼µæª¢ä¿®é …ç›®æ¸…å–®ã€‚è«‹ä»”ç´°è§€å¯Ÿæ¯ä¸€é …å‰é¢çš„æ–¹æ¡† (â–¡)ã€‚

åˆ—å‡ºæ‰€æœ‰ã€æ–¹æ¡†å…§æœ‰æ‰“å‹¾ (âœ“, v)ã€‘æˆ–ã€è¢«å¡—é»‘ã€‘çš„é …ç›®åç¨±ã€‚

IMPORTANT: Do NOT output any markdown, explanations, or code blocks. Only output a valid JSON array of strings.

Example: ["æ»…ç«å™¨", "é¿é›£å™¨å…·", "ç«è­¦è‡ªå‹•è­¦å ±è¨­å‚™"]

å¦‚æœæ²’æœ‰ä»»ä½•é …ç›®è¢«å‹¾é¸ï¼Œè«‹å›å‚³ç©ºé™£åˆ—: []"""
        
        payload = {
            "model": model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt,
                    "images": [img_base64]
                }
            ],
            "stream": False
        }
        
        response = requests.post(OLLAMA_CHAT_URL, json=payload, timeout=60)
        
        if response.status_code == 200:
            result = response.json()
            content = result['message']['content'].strip()
            
            # ä½¿ç”¨ Regex æå– JSON é™£åˆ—ï¼ˆé˜²æ­¢æ¨¡å‹è¼¸å‡ºå¤šé¤˜æ–‡å­—ï¼‰
            json_match = re.search(r'\[.*\]', content, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                checked_items = json.loads(json_str)
                return checked_items
            else:
                # å¦‚æœæ‰¾ä¸åˆ° JSONï¼Œå›å‚³ç©ºåˆ—è¡¨
                print(f"ç„¡æ³•å¾å›æ‡‰ä¸­æå– JSON: {content}")
                return []
        else:
            return []
            
    except Exception as e:
        print(f"å‹¾é¸é …ç›®æå–å¤±æ•—: {e}")
        return []

def analyze_document_structure(pdf_images_or_path, model=DEFAULT_VISION_MODEL):
    """
    å®Œæ•´çš„æ–‡ä»¶çµæ§‹åˆ†æ (4-step æµç¨‹)
    
    Args:
        pdf_images_or_path: å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€:
            - list of PIL Images
            - list of image file paths
            - PDF file path (æœƒè‡ªå‹•è½‰æ›ç‚ºåœ–ç‰‡)
        model (str): Vision æ¨¡å‹åç¨±
        
    Returns:
        dict: {
            'page_map': {é ç¢¼: æ–‡ä»¶é¡å‹},
            'toc_page': ç›®éŒ„é ç¢¼ (or None),
            'required_items': [å·²å‹¾é¸é …ç›®åˆ—è¡¨],
            'validation_report': pd.DataFrame,
            'error': error message (if any)
        }
    """
    # ç’°å¢ƒæª¢æŸ¥
    if not is_ollama_available():
        raise RuntimeError("Ollama æœå‹™æœªå•Ÿå‹•ï¼Œè«‹å…ˆåŸ·è¡Œ 'ollama serve' æˆ–å•Ÿå‹• Ollama Desktop")
    
    if not check_vision_model_available(model):
        raise RuntimeError(f"Vision æ¨¡å‹ '{model}' æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: ollama pull {model}")
    
    # è™•ç†è¼¸å…¥ (å¦‚æœæ˜¯ PDF è·¯å¾‘ï¼Œéœ€è¦è½‰æ›ç‚ºåœ–ç‰‡)
    # é€™è£¡å‡è¨­å·²ç¶“ç”±èª¿ç”¨æ–¹è½‰æ›å¥½ï¼ˆå› ç‚ºä¸»ç¨‹å¼å·²æœ‰è½‰æ›é‚è¼¯ï¼‰
    if isinstance(pdf_images_or_path, (str, Path)):
        # å¦‚æœå‚³å…¥çš„æ˜¯ PDF pathï¼Œé€™è£¡å¯ä»¥æ·»åŠ  pdf2image è½‰æ›é‚è¼¯
        # ä½†ç‚ºäº†ç°¡åŒ–ï¼Œæˆ‘å€‘å‡è¨­ä¸»ç¨‹å¼å·²è™•ç†å¥½åœ–ç‰‡åˆ—è¡¨
        raise NotImplementedError("è«‹å…ˆå°‡ PDF è½‰ç‚ºåœ–ç‰‡åˆ—è¡¨å†å‚³å…¥")
    
    images = pdf_images_or_path
    
    # çµæœå®¹å™¨
    result = {
        'page_map': {},
        'toc_page': None,
        'required_items': [],
        'validation_report': None,
        'error': None
    }
    
    try:
        # Step 1: é é¢è­˜åˆ¥ (Page Classification)
        print("ğŸ” Step 1: æ­£åœ¨é€²è¡Œé é¢è­˜åˆ¥...")
        for i, img in enumerate(images):
            page_num = i + 1
            
            # å¦‚æœæ˜¯ PIL Imageï¼Œéœ€è¦å…ˆå„²å­˜ç‚ºè‡¨æ™‚æª”æ¡ˆ
            if hasattr(img, 'save'):
                import os
                import tempfile
                temp_dir = tempfile.gettempdir()
                temp_path = os.path.join(temp_dir, f"temp_page_{page_num}.png")
                img.save(temp_path)
                doc_type = classify_page_with_vision(temp_path, model)
                try:
                    os.remove(temp_path)
                except:
                    pass  # å¿½ç•¥åˆªé™¤éŒ¯èª¤
            else:
                # å‡è¨­æ˜¯æª”æ¡ˆè·¯å¾‘
                doc_type = classify_page_with_vision(img, model)
            
            result['page_map'][page_num] = doc_type
            print(f"  ç¬¬ {page_num} é : {doc_type}")
        
        # Step 2: åµæ¸¬ã€Œæª¢ä¿®ç›®éŒ„ã€èˆ‡å‹¾é¸é …ç›®
        print("\nğŸ“‹ Step 2: æ­£åœ¨å°‹æ‰¾ç›®éŒ„é ä¸¦æå–å‹¾é¸é …ç›®...")
        
        # å°‹æ‰¾ç›®éŒ„é 
        toc_keywords = ['ç›®éŒ„', 'æª¢ä¿®é …ç›®', 'ç”³å ±é …ç›®', 'æ¸…å–®']
        for page_num, doc_type in result['page_map'].items():
            if any(keyword in doc_type for keyword in toc_keywords):
                result['toc_page'] = page_num
                print(f"  âœ… æ‰¾åˆ°ç›®éŒ„é : ç¬¬ {page_num} é ")
                
                # æå–å‹¾é¸é …ç›®
                img = images[page_num - 1]
                if hasattr(img, 'save'):
                    import os
                    import tempfile
                    temp_dir = tempfile.gettempdir()
                    temp_path = os.path.join(temp_dir, "temp_toc.png")
                    img.save(temp_path)
                    required_items = extract_checked_items_with_vision(temp_path, model)
                    try:
                        os.remove(temp_path)
                    except:
                        pass
                else:
                    required_items = extract_checked_items_with_vision(img, model)
                
                result['required_items'] = required_items
                print(f"  æ‰¾åˆ° {len(required_items)} å€‹å‹¾é¸é …ç›®: {required_items}")
                break
        
        if not result['toc_page']:
            print("  âš ï¸ æœªæ‰¾åˆ°ç›®éŒ„é ")
        
        # Step 3 & 4: äº¤å‰æ¯”å° & ç”Ÿæˆå ±å‘Š
        print("\nâœ… Step 3 & 4: æ­£åœ¨é€²è¡Œäº¤å‰æ¯”å°ä¸¦ç”Ÿæˆå ±å‘Š...")
        
        report_data = []
        for item in result['required_items']:
            # åˆ¤å®šè¦å‰‡: åœ¨ page_map ä¸­å°‹æ‰¾åŒ…å«è©²é …ç›®åç¨±çš„é é¢
            found_pages = []
            for page_num, doc_type in result['page_map'].items():
                # æ¨¡ç³ŠåŒ¹é… (ä¾‹å¦‚ "æ»…ç«å™¨" æ‡‰è©²åŒ¹é… "æ»…ç«å™¨æª¢æŸ¥è¡¨")
                if item in doc_type or doc_type in item:
                    found_pages.append(page_num)
            
            status = "âœ… åˆè¦" if found_pages else "âŒ ç¼ºä»¶"
            page_list = ", ".join([f"ç¬¬{p}é " for p in found_pages]) if found_pages else "-"
            
            report_data.append({
                'æ‡‰æª¢é™„é …ç›®': item,
                'æ˜¯å¦å‹¾é¸': 'âœ“',
                'å¯¦éš›é æ•¸': page_list,
                'ç‹€æ…‹': status
            })
        
        result['validation_report'] = pd.DataFrame(report_data)
        
        print("  âœ… å ±å‘Šç”Ÿæˆå®Œæˆ")
        return result
        
    except Exception as e:
        result['error'] = str(e)
        print(f"âŒ åˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        return result

# === èˆŠç‰ˆæ–‡å­—åˆ†æå‡½å¼ (ä¿ç•™å‘å¾Œå…¼å®¹) ===
def analyze_page_with_ai(text_content, model=DEFAULT_TEXT_MODEL):
    """
    ä½¿ç”¨ AI åˆ†æå–®é å…§å®¹ (åŸºæ–¼æ–‡å­—çš„ OCR çµæœ)
    
    Args:
        text_content (str): OCR è¾¨è­˜å‡ºçš„æ–‡å­—
        model (str): ä½¿ç”¨çš„æ¨¡å‹åç¨±
        
    Returns:
        dict: AI åˆ†æçµæœ
    """
    if not text_content.strip():
        return {"error": "No text content"}

    prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ¶ˆé˜²å®‰å…¨æª¢æŸ¥å“¡ã€‚è«‹å¾ä»¥ä¸‹ OCR æ–‡å­—ä¸­æå–é—œéµè³‡è¨Šã€‚
    
    OCR æ–‡å­—:
    ----------------
    {text_content}
    ----------------
    
    è«‹æå–ä»¥ä¸‹æ¬„ä½ä¸¦ä»¥ JSON æ ¼å¼å›å‚³ã€‚
    
    âš ï¸ é‡è¦è¦å‰‡ï¼š
    1. **å»é™¤æ‰€æœ‰ç©ºæ ¼**ï¼šæ‰€æœ‰è¼¸å‡ºçš„å€¼éƒ½å¿…é ˆå»é™¤æ‰€æœ‰ç©ºæ ¼ (ä¾‹å¦‚ "é³³ ä»™" -> "é³³ä»™")ã€‚
    2. **å–®ä¸€å­—ä¸²**ï¼šåœ°å€å’Œç®¡ç†æ¬Šäººå¿…é ˆæ˜¯å–®ä¸€å­—ä¸²ï¼Œåš´ç¦ä½¿ç”¨å·¢ç‹€ JSON (ä¾‹å¦‚ä¸è¦å›å‚³ {{'city': ...}})ã€‚
    
    æ¬„ä½èªªæ˜ï¼š
    1. document_type: æ–‡ä»¶é¡å‹
    2. place_name: å ´æ‰€åç¨± (å»é™¤ç©ºæ ¼)
    3. address: åœ°å€ (å®Œæ•´åœ°å€å­—ä¸²ï¼Œå»é™¤ç©ºæ ¼)
    4. management_person: ç®¡ç†æ¬Šäºº (å§“åå­—ä¸²ï¼Œå»é™¤ç©ºæ ¼)
    5. equipment_list: æ¶ˆé˜²è¨­å‚™åˆ—è¡¨ (Arrayï¼Œæ¯å€‹é …ç›®ä¹Ÿè¦å»é™¤ç©ºæ ¼)

    å¦‚æœæ‰¾ä¸åˆ°æ¬„ä½ï¼Œè«‹å¡« nullã€‚åªå›å‚³ JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
    """
    
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    
    try:
        response = requests.post(OLLAMA_GENERATE_URL, json=payload, timeout=30)
        if response.status_code == 200:
            result = response.json()
            response_text = result['response']
            
            # å˜—è©¦ä½¿ç”¨ Regex æå– JSON (è™•ç† Markdown æ¨™è¨˜)
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
            else:
                # å¦‚æœ Regex å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥è§£æ
                return json.loads(response_text)
        else:
            return {"error": f"API Error: {response.status_code}"}
    except Exception as e:
        return {"error": str(e)}

def analyze_document(pages_text, model=DEFAULT_TEXT_MODEL):
    """
    åˆ†ææ•´ä»½æ–‡ä»¶ (å¤šé ) - åŸºæ–¼ OCR æ–‡å­—
    
    Args:
        pages_text (list): æ¯ä¸€é çš„ OCR æ–‡å­—åˆ—è¡¨
        
    Returns:
        dict: æ•´åˆå¾Œçš„åˆ†æçµæœ
    """
    if not is_ollama_available():
        return {"error": "Ollama service not available"}
        
    # é€™è£¡å¯ä»¥å¯¦ä½œæ›´è¤‡é›œçš„é‚è¼¯ï¼Œä¾‹å¦‚åªåˆ†æç¬¬ä¸€é ï¼Œæˆ–æ˜¯å½™æ•´æ‰€æœ‰é é¢
    # å„ªåŒ–ï¼šåŒæ™‚åˆ†æç¬¬ä¸€é (åŸºæœ¬è³‡æ–™)å’Œç›®éŒ„é (è¨­å‚™æ¸…å–®)
    if pages_text:
        combined_text = pages_text[0] # é è¨­åŒ…å«ç¬¬ä¸€é 
        
        # å°‹æ‰¾ç›®éŒ„é  (é—œéµå­—: ç›®éŒ„, é™„è¡¨, æª¢æŸ¥è¡¨)
        toc_keywords = ["ç›®éŒ„", "é™„è¡¨", "æª¢æŸ¥è¡¨"]
        toc_text = ""
        
        # å¾ç¬¬äºŒé é–‹å§‹æ‰¾ (index 1)
        if len(pages_text) > 1:
            for i in range(1, len(pages_text)):
                page_content = pages_text[i]
                # ç°¡å–®åˆ¤æ–·
                if any(kw in page_content for kw in toc_keywords):
                    toc_text = page_content
                    break
            
            # å¦‚æœæ²’æ‰¾åˆ°æ˜ç¢ºçš„ç›®éŒ„é ï¼Œä½†æœ‰ç¬¬äºŒé ï¼Œå°±é è¨­æŠ“ç¬¬äºŒé  (é€šå¸¸ç›®éŒ„åœ¨ç¬¬äºŒé )
            if not toc_text and len(pages_text) > 1:
                toc_text = pages_text[1]
        
        if toc_text:
            combined_text += "\n\n--- (ä»¥ä¸‹ç‚ºç›®éŒ„é å…§å®¹) ---\n\n" + toc_text
            
        return analyze_page_with_ai(combined_text, model)
    return {}
